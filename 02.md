02: Some basic considerations concerning estimation in clinical trials
================
A Solomon Kurz
2023-11-20

## 2.1 The purpose of this chapter

Senn opened:

> The purpose of this chapter is to review various basic statistical
> concepts regarding clinical trials which will either be referred to
> subsequently or assumed as general background knowledge. (p. 17)

## 2.2 Assumed background knowledge

## 2.3 Control in clinical trials

This section is worth a read, particularly for those not familiar with
the contemporary causal inference paradigm.

## 2.4 Two purposes of estimation

We want to assess the causal estimand for those in the trial, and to
make inferences for other potential persons in the future. Both are
hard.

## 2.5 Some features of estimation

In pages 28-29, we lean that given

$$
Y_i \sim \mathcal N(\mu, \sigma),
$$

when we take some finite sample $n$, the sample variance for the point
estimate of the mean, $\bar y$, is $\hat \sigma^2 / n$. Let’s see this
in action.

``` r
# load
library(tidyverse)
library(broom)
library(marginaleffects)

# how many do you want?
n <- 1000

# simulate
set.seed(1)
d <- tibble(y = rnorm(n = n))
```

The standard error for $\hat \mu$ is 0.03273. Here’s the population
value for $\sigma^2 / n$.

``` r
1 / n
```

    ## [1] 0.001

Here’s the sample value.

``` r
d %>% 
  summarise(var_y_bar = var(y) / n())
```

    ## # A tibble: 1 × 1
    ##   var_y_bar
    ##       <dbl>
    ## 1   0.00107

Now notice what happens when we fit an intercept-only model to the data.

``` r
# fit
fit2.1 <- lm(
  data = d,
  y ~ 1
)

# summarize
summary(fit2.1)
```

    ## 
    ## Call:
    ## lm(formula = y ~ 1, data = d)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -2.9964 -0.6857 -0.0237  0.7001  3.8219 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)
    ## (Intercept) -0.01165    0.03273  -0.356    0.722
    ## 
    ## Residual standard error: 1.035 on 999 degrees of freedom

See the standard error for $\beta_0$? Here’s what happens when we square
that value, placing it in a variance metric.

``` r
vcov(fit2.1) %>% as.double()
```

    ## [1] 0.001071051

That’s the same as our hand-computed `var_y_bar` value, above. Here’s
another way to pull that value from the model.

``` r
sigma(fit2.1)^2 / n
```

    ## [1] 0.001071051

``` r
# 2 treatments
n_tx <- 2

# 2 centers 
n_center <- 2

# each center recruits toward the goal of n patients (in total); BUT
# we do not assume the two centers have the same sizes
n <- 1000

# we assume equal numbers in the 2 levels of tx

# we assume constant variance (SD)
sigma_e <- 1

# we start with a simple model with constant ATE by center
tau <- 1

# here's the control mean
beta0 <- 0

# simulate
set.seed(2)

d <- tibble(
  id = 1:n,
  # we assume equal numbers in the 2 levels of tx
  tx = rep(0:1, each = n / n_tx) %>% 
    sample(size = n, replace = FALSE),
  # center is totally random, and does not presume equality of size
  center = (0:1) %>% sample(size = n, replace = TRUE)
) %>% 
  mutate(y = rnorm(n = n,
                   mean = beta0 + tau * tx, 
                   sd = sigma_e))

# what?
head(d)
```

    ## # A tibble: 6 × 4
    ##      id    tx center      y
    ##   <int> <int>  <int>  <dbl>
    ## 1     1     1      1 -0.301
    ## 2     2     1      1  0.747
    ## 3     3     1      0  1.97 
    ## 4     4     1      0  0.789
    ## 5     5     0      1 -1.39 
    ## 6     6     0      1  1.98

The sample sizes are equal by `tx`.

``` r
d %>% 
  count(tx)
```

    ## # A tibble: 2 × 2
    ##      tx     n
    ##   <int> <int>
    ## 1     0   500
    ## 2     1   500

However, they are only probabilistically equal by `center`, and thus
similarily so by `center` and `tx`.

``` r
d %>% 
  count(center)
```

    ## # A tibble: 2 × 2
    ##   center     n
    ##    <int> <int>
    ## 1      0   477
    ## 2      1   523

``` r
d %>% 
  count(center, tx)
```

    ## # A tibble: 4 × 3
    ##   center    tx     n
    ##    <int> <int> <int>
    ## 1      0     0   243
    ## 2      0     1   234
    ## 3      1     0   257
    ## 4      1     1   266

Here are the grouped sample means and SDs.

``` r
d %>% 
  group_by(tx, center) %>% 
  summarise(m = mean(y),
            s = sd(y))
```

    ## `summarise()` has grouped output by 'tx'. You can override using the `.groups`
    ## argument.

    ## # A tibble: 4 × 4
    ## # Groups:   tx [2]
    ##      tx center      m     s
    ##   <int>  <int>  <dbl> <dbl>
    ## 1     0      0 0.0248 1.05 
    ## 2     0      1 0.0953 1.05 
    ## 3     1      0 0.993  0.985
    ## 4     1      1 1.01   0.971

This is following the special case where

$$
\mu_\textit{IB} - \mu_\textit{IA} = \mu_\textit{IIB} - \mu_\textit{IIA} = \tau,
$$

where the Roman letters $\textit{I}$ and $\textit{II}$ indicate the two
levels of `center` and the other Roman letters $\textit{A}$ and
$\textit{B}$ indicate the two levels of `tx`. The average treatment
effect (ATE), of course, is depicted by $\tau$.

One way we might estimate the ATE is “by using the mean in each
treatment group over both cent\[er\]s” (p. 30), which returns

$$
\hat \tau_1 = \bar Y_\textit{.B.} - \bar Y_\textit{.A.}.
$$

Here presumably *over both cent\[er\]s* means ignoring `center`. Here’s
that in code.

``` r
d %>% 
  group_by(tx) %>% 
  summarise(m = mean(y)) %>% 
  pivot_wider(names_from = tx, values_from = m) %>% 
  mutate(tau_1 = `1` - `0`)
```

    ## # A tibble: 1 × 3
    ##      `0`   `1` tau_1
    ##    <dbl> <dbl> <dbl>
    ## 1 0.0610  1.00 0.940

This estimator $\hat \tau_1$ is unbiased only presuming

- equal sample sizes between centers (which we don’t have); or
- equality of means by `tx` across centers
  - that is $\mu_\textit{IA} = \mu_\textit{IIA}$ and
    $\mu_\textit{IB} = \mu_\textit{IIB}$;
  - which we only have at the population level, but not in the sample.

Let’s reproduce our sample-statistic approach with a model.

``` r
fit2.2 <- lm(
  data = d,
  y ~ tx
)

summary(fit2.2)
```

    ## 
    ## Call:
    ## lm(formula = y ~ tx, data = d)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -3.2027 -0.6674  0.0210  0.6458  3.4789 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  0.06103    0.04530   1.347    0.178    
    ## tx           0.93952    0.06406  14.665   <2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 1.013 on 998 degrees of freedom
    ## Multiple R-squared:  0.1773, Adjusted R-squared:  0.1765 
    ## F-statistic: 215.1 on 1 and 998 DF,  p-value: < 2.2e-16

The $\hat \beta_1$ value is the same as our hand-computed $\hat \tau_1$
value. Here’s a simulation looking at the properties of this approach.

``` r
sim_tau1 <- function(seed = 1, n = 100, tau = 1, beta0 = 0, sigma_e = 1) {
  
  # 2 treatments
  n_tx <- 2
  
  # each center recruits toward the goal of n patients (in total); BUT
  # we do not assume the two centers have the same sizes
  n <- n
  
  # we assume equal numbers in the 2 levels of tx
  
  # we assume constant variance (SD)
  sigma_e <- sigma_e
  
  # we start with a simple model with constant ATE by center
  tau <- tau
  
  # here's the control mean
  beta0 <- beta0
  
  # simulate
  set.seed(seed)
  
  d <- tibble(
    id = 1:n,
    # we assume equal numbers in the 2 levels of tx
    tx = rep(0:1, each = n / n_tx) %>% 
      sample(size = n, replace = FALSE),
    # center is totally random, and does not presume equality of size
    center = (0:1) %>% sample(size = n, replace = TRUE)
  ) %>% 
    mutate(y = rnorm(n = n, mean = beta0 + tau * tx, sd = sigma_e))
  
  # fit
  fit_sim <- lm(
    data = d,
    y ~ tx
  )
  
  # summarize
  tidy(fit_sim) %>% slice(2)
  
}
```

``` r
sim_tau1(seed = 1, n = 100, tau = 1, beta0 = 0, sigma_e = 1)
```

    ## # A tibble: 1 × 5
    ##   term  estimate std.error statistic     p.value
    ##   <chr>    <dbl>     <dbl>     <dbl>       <dbl>
    ## 1 tx        1.11     0.206      5.41 0.000000454

``` r
# 1.715423 secs
t0 <- Sys.time()
sim_n100_i1000 <- tibble(seed = 1:1000) %>% 
  mutate(tidy = map(.x = seed, .f = sim_tau1)) %>% 
  unnest(tidy)

t1 <- Sys.time()
t1 - t0
```

    ## Time difference of 1.436177 secs

What’s the bias?

``` r
sim_n100_i1000 %>% 
  summarise(m = mean(estimate),
            b = 1 - abs(mean(estimate)),
            sd_e = sd(estimate))
```

    ## # A tibble: 1 × 3
    ##       m         b  sd_e
    ##   <dbl>     <dbl> <dbl>
    ## 1  1.00 -0.000778 0.196

The bias is minimal, even with $N = 100$.

The population variance for the $\tau_1$ estimator is $2 \sigma^2 / n$.
Here’s that value, and it’s transformation into a standard-deviation
metric, for the kind of data in our simulation based on $N = 100$.

``` r
sqrt(2 * 1^2 / 100)
```

    ## [1] 0.1414214

We have a upward bias in the variance (i.e., an inefficiency issue).

``` r
fit2.3 <- lm(
  data = d,
  y ~ tx + center
)

summary(fit2.3)
```

    ## 
    ## Call:
    ## lm(formula = y ~ tx + center, data = d)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -3.2234 -0.6727  0.0202  0.6503  3.4581 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  0.03906    0.05604   0.697    0.486    
    ## tx           0.93875    0.06409  14.647   <2e-16 ***
    ## center       0.04273    0.06416   0.666    0.506    
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 1.013 on 997 degrees of freedom
    ## Multiple R-squared:  0.1777, Adjusted R-squared:  0.176 
    ## F-statistic: 107.7 on 2 and 997 DF,  p-value: < 2.2e-16

``` r
vcov(fit2.2)["tx", "tx"]
```

    ## [1] 0.004104142

``` r
vcov(fit2.3)["tx", "tx"]
```

    ## [1] 0.004107766

But anyway, the big thing to worry about with this method is when there
is a systemic between centers such that

$$
\mu_\textit{IIA} - \mu_\textit{IA} = \mu_\textit{IIB} - \mu_\textit{IB} = \delta,
$$

where the between-`center` difference is the same within levels of the
experimental condition `tx`. There are special cases where $\tau_1$ is
still an unbiased estimator for this, but we can instead construct a
different estimator as

$$
\hat \tau_2 = \frac{(\bar Y_\textit{IB.} - \bar Y_\textit{IA.}) + (\bar Y_\textit{IIB.} - \bar Y_\textit{IIA.})}{2},
$$

which will work even with unbalanced cell sizes.

Let’s simulate new `d` data accomodating our new $\delta$ parameter.

``` r
# 2 treatments
n_tx <- 2

# 2 centers 
n_center <- 2

# each center recruits toward the goal of n patients (in total); BUT
# we do not assume the two centers have the same sizes
n <- 1000

# we assume equal numbers in the 2 levels of tx

# we assume constant variance (SD)
sigma_e <- 1

# we start with a simple model with constant ATE by center
tau <- 1

# here's the control mean
beta0 <- 0

# here's the center difference
delta <- 0.5

# simulate
set.seed(2)

d <- tibble(
  id = 1:n,
  # we assume equal numbers in the 2 levels of tx
  tx = rep(0:1, each = n / n_tx) %>% 
    sample(size = n, replace = FALSE),
  # center is totally random, and does not presume equality of size
  center = (0:1) %>% sample(size = n, replace = TRUE)
) %>% 
  mutate(y = rnorm(n = n,
                   mean = beta0 + tau * tx + delta * center, 
                   sd = sigma_e))

# what?
head(d)
```

    ## # A tibble: 6 × 4
    ##      id    tx center      y
    ##   <int> <int>  <int>  <dbl>
    ## 1     1     1      1  0.199
    ## 2     2     1      1  1.25 
    ## 3     3     1      0  1.97 
    ## 4     4     1      0  0.789
    ## 5     5     0      1 -0.892
    ## 6     6     0      1  2.48

Here are the new grouped sample means and SDs.

``` r
d %>% 
  group_by(tx, center) %>% 
  summarise(m = mean(y),
            s = sd(y))
```

    ## `summarise()` has grouped output by 'tx'. You can override using the `.groups`
    ## argument.

    ## # A tibble: 4 × 4
    ## # Groups:   tx [2]
    ##      tx center      m     s
    ##   <int>  <int>  <dbl> <dbl>
    ## 1     0      0 0.0248 1.05 
    ## 2     0      1 0.595  1.05 
    ## 3     1      0 0.993  0.985
    ## 4     1      1 1.51   0.971

Here are the grouped estimates for $\tau$.

``` r
d %>% 
  group_by(tx, center) %>% 
  summarise(m = mean(y)) %>% 
  pivot_wider(names_from = tx, values_from = m) %>% 
  mutate(tau = `1` - `0`)
```

    ## `summarise()` has grouped output by 'tx'. You can override using the `.groups`
    ## argument.

    ## # A tibble: 2 × 4
    ##   center    `0`   `1`   tau
    ##    <int>  <dbl> <dbl> <dbl>
    ## 1      0 0.0248 0.993 0.968
    ## 2      1 0.595  1.51  0.912

Here are the grouped estimates for $\delta$ (not presuming equality of
that difference within levels of `tx`).

``` r
d %>% 
  group_by(tx, center) %>% 
  summarise(m = mean(y)) %>% 
  pivot_wider(names_from = center, values_from = m) %>% 
  mutate(delta = `1` - `0`)
```

    ## `summarise()` has grouped output by 'tx'. You can override using the `.groups`
    ## argument.

    ## # A tibble: 2 × 4
    ## # Groups:   tx [2]
    ##      tx    `0`   `1` delta
    ##   <int>  <dbl> <dbl> <dbl>
    ## 1     0 0.0248 0.595 0.570
    ## 2     1 0.993  1.51  0.515

Here’s $\hat \tau_2$, by hand.

``` r
d %>% 
  group_by(tx, center) %>% 
  summarise(m = mean(y)) %>% 
  pivot_wider(names_from = tx, values_from = m) %>% 
  mutate(tau = `1` - `0`) %>% 
  summarise(tau_2 = sum(tau) / 2)
```

    ## `summarise()` has grouped output by 'tx'. You can override using the `.groups`
    ## argument.

    ## # A tibble: 1 × 1
    ##   tau_2
    ##   <dbl>
    ## 1 0.940

If we mean-center the `center` variable, we fit an ANHECOVA model where
$\beta_1$ is essentially an estimator for $\tau_2$.

``` r
fit2.4 <- lm(
  data = d %>% mutate(centerc = center - mean(center)),
  y ~ 1 + tx + center + tx : centerc
)

summary(fit2.4)
```

    ## 
    ## Call:
    ## lm(formula = y ~ 1 + tx + center + tx:centerc, data = d %>% mutate(centerc = center - 
    ##     mean(center)))
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -3.2369 -0.6612  0.0245  0.6548  3.4446 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  0.02483    0.06502   0.382    0.703    
    ## tx           0.93875    0.06412  14.641  < 2e-16 ***
    ## center       0.57043    0.09070   6.289 4.77e-10 ***
    ## tx:centerc  -0.05548    0.12837  -0.432    0.666    
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 1.014 on 996 degrees of freedom
    ## Multiple R-squared:  0.2259, Adjusted R-squared:  0.2235 
    ## F-statistic: 96.87 on 3 and 996 DF,  p-value: < 2.2e-16

Here’s the ANHECOVA-based $\hat \tau$ using the `avg_comparisons()`
approach.

``` r
avg_comparisons(fit2.4, variables = "tx") %>% 
  data.frame()
```

    ##   term contrast  estimate std.error statistic      p.value  s.value  conf.low
    ## 1   tx    1 - 0 0.9387474  0.064118  14.64093 1.539409e-48 158.8302 0.8130784
    ##   conf.high
    ## 1  1.064416

You’ll note that the results are the same as just using the $\beta_1$
coefficient. These differ from $\tau_2$ in the text because these are
weighted, whereas the formula for $\tau_2$ (Equation 2.24) is not
weighted.

Here’s the `avg_comparisons()`-based estimate for $\delta$.

``` r
avg_comparisons(fit2.4, variables = "center") %>% 
  data.frame()
```

    ##     term contrast  estimate  std.error statistic      p.value  s.value
    ## 1 center    1 - 0 0.5704252 0.09069733  6.289328 3.188441e-10 31.54643
    ##    conf.low conf.high
    ## 1 0.3926617 0.7481887

If we want to replicate the un-weighted $\hat \tau_2$ approach with
regression, we’ll need to instead use the Oaxaca-blinder type approach
where we fit the models separately by `center`, and then take the
averages of their two $\hat \beta_1$ values.

``` r
# fit the 2 models
fit2.5 <- lm(
  data = d %>% filter(center == 0),
  y ~ 1 + tx
)

fit2.6 <- lm(
  data = d %>% filter(center == 1),
  y ~ 1 + tx
)

# summarize for fun
summary(fit2.5)
```

    ## 
    ## Call:
    ## lm(formula = y ~ 1 + tx, data = d %>% filter(center == 0))
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -2.86464 -0.72117  0.02198  0.66839  2.81495 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  0.02483    0.06531    0.38    0.704    
    ## tx           0.96776    0.09324   10.38   <2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 1.018 on 475 degrees of freedom
    ## Multiple R-squared:  0.1849, Adjusted R-squared:  0.1832 
    ## F-statistic: 107.7 on 1 and 475 DF,  p-value: < 2.2e-16

``` r
summary(fit2.6)
```

    ## 
    ## Call:
    ## lm(formula = y ~ 1 + tx, data = d %>% filter(center == 1))
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -3.2369 -0.6296  0.0299  0.6140  3.4446 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  0.59525    0.06298   9.452   <2e-16 ***
    ## tx           0.91228    0.08831  10.331   <2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 1.01 on 521 degrees of freedom
    ## Multiple R-squared:   0.17,  Adjusted R-squared:  0.1684 
    ## F-statistic: 106.7 on 1 and 521 DF,  p-value: < 2.2e-16

Now compute our version of $\hat \tau_2$.

``` r
(coef(fit2.5)["tx"] + coef(fit2.6)["tx"]) / 2
```

    ##        tx 
    ## 0.9400234

If you look back, this is the same value we computed from the sample
statistics. But also notice that this Oaxaca-blinder type approach does
not impose the constant $\sigma^2$ assumption.

## 2.6 Practical consequences for cross-over trials

This section has more to do with planning a cross-over trial, than with
how to analyze the data from such a trial.

## Session info

``` r
sessionInfo()
```

    ## R version 4.3.1 (2023-06-16)
    ## Platform: aarch64-apple-darwin20 (64-bit)
    ## Running under: macOS Ventura 13.4
    ## 
    ## Matrix products: default
    ## BLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib 
    ## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0
    ## 
    ## locale:
    ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
    ## 
    ## time zone: America/Chicago
    ## tzcode source: internal
    ## 
    ## attached base packages:
    ## [1] stats     graphics  grDevices utils     datasets  methods   base     
    ## 
    ## other attached packages:
    ##  [1] marginaleffects_0.16.0 broom_1.0.5            lubridate_1.9.2       
    ##  [4] forcats_1.0.0          stringr_1.5.0          dplyr_1.1.2           
    ##  [7] purrr_1.0.1            readr_2.1.4            tidyr_1.3.0           
    ## [10] tibble_3.2.1           ggplot2_3.4.3          tidyverse_2.0.0       
    ## 
    ## loaded via a namespace (and not attached):
    ##  [1] gtable_0.3.4      compiler_4.3.1    Rcpp_1.0.11       tidyselect_1.2.0 
    ##  [5] scales_1.2.1      yaml_2.3.7        fastmap_1.1.1     R6_2.5.1         
    ##  [9] generics_0.1.3    knitr_1.43        backports_1.4.1   checkmate_2.2.0  
    ## [13] insight_0.19.6    munsell_0.5.0     pillar_1.9.0      tzdb_0.4.0       
    ## [17] rlang_1.1.1       utf8_1.2.3        stringi_1.7.12    xfun_0.40        
    ## [21] timechange_0.2.0  cli_3.6.1         withr_2.5.1       magrittr_2.0.3   
    ## [25] digest_0.6.33     grid_4.3.1        rstudioapi_0.14   hms_1.1.3        
    ## [29] lifecycle_1.0.3   vctrs_0.6.3       data.table_1.14.8 evaluate_0.21    
    ## [33] glue_1.6.2        fansi_1.0.4       colorspace_2.1-0  rmarkdown_2.24   
    ## [37] tools_4.3.1       pkgconfig_2.0.3   htmltools_0.5.6
